{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9711879",
   "metadata": {},
   "source": [
    "# Arithmetic conversion: *folding* and *casting*\n",
    "\n",
    "Apart from graph rewriting, fake-to-true conversions require reordering arithmetic operations, folding together some operands, and casting the results to integer or other low-precision data formats, while at the same time preserving functional equivalence (or at least minimising the discrepancy between the original version and the converted one).\n",
    "We refer to this part of the conversion as *arithmetic conversion*.\n",
    "\n",
    "\n",
    "### *Folding*\n",
    "\n",
    "We call *folding* each sequence of applications of basic arithmetical properties or identity decompositions used to transform a given sequence of operations into an equivalent one.\n",
    "Commonly-used arithmetical properties are:\n",
    "\n",
    "* the *commutative* property: $a + b = b + a$;\n",
    "* the *associative* property: $(a + b) + c = a + (b + c)$;\n",
    "* the *distributive* property and its inverse: $(a + b) * c = (a * c) + (b * c)$ and $(a * c) + (b * c) = (a + b) * c$.\n",
    "\n",
    "Given a non-zero number $z \\in \\mathbb{R}$, the identity decompositions are the following:\n",
    "\n",
    "* *sum-and-subtract*: $x = x - z + z$;\n",
    "* *divide-and-multiply*: $ x = (x / z) * z$.\n",
    "\n",
    "For example, consider the operation of normalising a scalar product.\n",
    "Let $n > 0$ be an integer, $\\mathbb{R}^{n}$ be the $n$-dimensional Euclidean space, and $\\mathbf{x}, \\mathbf{w} \\in \\mathbb{R}^{n}$ be vectors.\n",
    "Let also $\\mu \\in \\mathbb{R},\\, \\sigma \\in \\mathbb{R}^{+}$, and consider the following operation:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\frac{\\langle \\mathbf{x}, \\mathbf{w} \\rangle - \\mu}{\\sigma} \\,.\n",
    "\\end{equation*}\n",
    "\n",
    "Folding rules (in this specific case, the distributive property) allow us to rewrite this operation as\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    \\frac{\\langle \\mathbf{x}, \\mathbf{w} \\rangle - \\mu}{\\sigma}\n",
    "    &= \\left( \\langle \\mathbf{x}, \\mathbf{w} \\rangle - \\mu \\right) \\frac{1}{\\sigma} \\\\\n",
    "    &= \\langle \\mathbf{x}, \\mathbf{w} \\rangle \\frac{1}{\\sigma} - \\frac{\\mu}{\\sigma} \\\\\n",
    "    &= \\langle \\mathbf{x}, \\mathbf{w} \\rangle \\tilde{\\sigma} + \\tilde{\\mu} \\,.\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "The critical point here is that the operands $\\tilde{\\sigma} = 1/\\sigma \\in \\mathbb{R}^{+}$ and $\\tilde{\\mu} = -\\mu/\\sigma$ can be **pre-computed** starting from the operands $\\mu$ and $\\sigma$.\n",
    "The original version of the operation involves computing the scalar product $\\langle \\mathbf{x}, \\mathbf{w} \\rangle = \\sum_{i=1}^{n} x_{i} w_{i}$ ($n$ multiplications and $n-1$ sums), one subtraction, and one division.\n",
    "By contrast, the folded version still requires computing the scalar product, but then uses a multiplication and an addition.\n",
    "\n",
    "In this case, we are still performing the same number of operations, although with different operands.\n",
    "In other cases, though, the benefits of folding can become more apparent.\n",
    "For example, instead of supposing $\\mathbf{x}, \\mathbf{w} \\in \\mathbb{R}^{n}$, we can imagine that the components of these vectors take values in specific spaces of the form\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb{Z}_{\\epsilon} := \\{ \\epsilon i \\,|\\, i \\in \\mathbb{Z} \\} \\,,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\epsilon \\in \\mathbb{R}^{+}$ is a positive constant.\n",
    "In particular, given constants $\\epsilon_{\\mathbf{x}}, \\epsilon_{\\mathbf{w}} > 0$ we assume that $\\mathbf{x} \\in \\mathbb{Z}_{\\epsilon_{\\mathbf{x}}}^{n}$ and $\\mathbf{w} \\in \\mathbb{Z}_{\\epsilon_{\\mathbf{w}}}^{n}$.\n",
    "Due to the definition of the spaces $\\mathbb{Z}_{\\epsilon}$, we can make explicit the fact that each $\\mathbf{x} \\in \\mathbb{Z}_{\\epsilon_{\\mathbf{x}}}^{n}$ can be rewritten as\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbf{x} = \\epsilon_{\\mathbf{x}} \\hat{\\mathbf{x}} \\,,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\hat{\\mathbf{x}} \\in \\mathbb{Z}^{n}$ if a vector with integer components $\\hat{x}_{i} \\in \\mathbb{Z}, \\, i = 1, \\dots, n$.\n",
    "An analogous representation is available for $\\mathbf{w} \\in \\mathbb{Z}_{\\epsilon_{\\mathbf{w}}}^{n}$: $\\mathbf{w} = \\epsilon_{\\mathbf{w}} \\hat{\\mathbf{w}}$.\n",
    "\n",
    "Given parameters $\\gamma, \\beta \\in \\mathbb{R}$ and a positive parameter $\\epsilon_{\\mathbf{s}} > 0$, a conversion that is often required in fake-to-true network transformations is the following:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    \\frac{\\left( \\frac{\\langle \\mathbf{x}, \\mathbf{w} \\rangle - \\mu}{\\sigma} \\right) \\gamma + \\beta}{\\epsilon_{\\mathbf{s}}}\n",
    "    &= \\frac{\\left( \\frac{\\langle \\epsilon_{\\mathbf{x}} \\hat{\\mathbf{x}}, \\epsilon_{\\mathbf{w}} \\hat{\\mathbf{w}} \\rangle - \\mu}{\\sigma} \\right) \\gamma + \\beta}{\\epsilon_{\\mathbf{s}}} \\\\\n",
    "    &= \\left( \\left( \\frac{ \\sum_{i=1}^{n} \\epsilon_{\\mathbf{x}} \\hat{x}_{i} \\epsilon_{\\mathbf{w}} \\hat{w}_{i} - \\mu}{\\sigma} \\right) \\gamma + \\beta \\right) \\frac{1}{\\epsilon_{\\mathbf{s}}} \\\\\n",
    "    &= \\left( \\left( \\frac{ \\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\sum_{i=1}^{n} \\hat{x}_{i} \\hat{w}_{i} - \\mu}{\\sigma} \\right) \\gamma + \\beta \\right) \\frac{1}{\\epsilon_{\\mathbf{s}}} \\\\\n",
    "    &= \\left( \\left( \\frac{ \\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\langle \\hat{\\mathbf{x}}, \\hat{\\mathbf{w}} \\rangle - \\mu}{\\sigma} \\right) \\gamma + \\beta \\right) \\frac{1}{\\epsilon_{\\mathbf{s}}} \\\\\n",
    "    &= \\left( \\langle \\hat{\\mathbf{x}}, \\hat{\\mathbf{w}} \\rangle \\left( \\frac{\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma}{\\sigma} \\right) + \\left( \\frac{\\sigma \\beta - \\mu \\gamma}{\\sigma} \\right) \\right) \\frac{1}{\\epsilon_{\\mathbf{s}}} \\,.\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "At this point, the folding process can evolve in two directions: the *add-then-multiply*, and the *multiply-then-add*.\n",
    "The *add-then-multiply* folding strategy divides the first factor by $(\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma)/\\sigma$ and multiplies the second by the same value, obtaining\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    \\frac{\\left( \\frac{\\langle \\mathbf{x}, \\mathbf{w} \\rangle - \\mu}{\\sigma} \\right) \\gamma + \\beta}{\\epsilon_{\\mathbf{s}}}\n",
    "    &= \\left( \\langle \\hat{\\mathbf{x}}, \\hat{\\mathbf{w}} \\rangle + \\left( \\frac{\\sigma \\beta - \\mu \\gamma}{\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma} \\right) \\right) \\frac{\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\sigma}{\\epsilon_{\\mathbf{s}} \\sigma} \\\\\n",
    "    &= \\left( \\langle \\hat{\\mathbf{x}}, \\hat{\\mathbf{w}} \\rangle + \\beta' \\right) \\gamma' \\,,\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\beta' = (\\sigma \\beta - \\mu \\gamma)/(\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma)$ and $\\sigma' = (\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma)/(\\epsilon_{\\mathbf{s}} \\sigma)$.\n",
    "The *multiply-then-add* folding strategy directly multiplies the first factor by $1/\\epsilon_{\\mathbf{s}}$, yielding\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    \\frac{\\left( \\frac{\\langle \\mathbf{x}, \\mathbf{w} \\rangle - \\mu}{\\sigma} \\right) \\gamma + \\beta}{\\epsilon_{\\mathbf{s}}}\n",
    "    &= \\langle \\hat{\\mathbf{x}}, \\hat{\\mathbf{w}} \\rangle \\left( \\frac{\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma}{\\epsilon_{\\mathbf{s}} \\sigma} \\right) + \\left( \\frac{\\sigma \\beta - \\mu \\gamma}{\\epsilon_{\\mathbf{x}} \\sigma} \\right) \\\\\n",
    "    &= \\langle \\hat{\\mathbf{x}}, \\hat{\\mathbf{w}} \\rangle \\gamma'' + \\beta'' \\,,\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\beta'' = (\\sigma \\beta - \\mu \\gamma)/(\\epsilon_{\\mathbf{s}} \\sigma)$ and $\\sigma'' = \\sigma' = (\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma)/(\\epsilon_{\\mathbf{s}} \\sigma)$.\n",
    "\n",
    "Before folding, the original expression involves $n$ products and $n-1$ sums between *decimal* numbers (assuming that $\\epsilon_{\\mathbf{x}}$ and $\\epsilon_{\\mathbf{w}}$ are decimal numbers), a subtraction, a division, a multiplication, an addition, and another division.\n",
    "After folding, the expression involves $n$ products and $n-1$ sums between *integer* numbers, a multiplication, and an addition.\n",
    "By folding, we have derived a more compact version of the original expression, saving three operations.\n",
    "Nevertheless, the main benefit from the perspective of digital arithmetic is that performing multiplications and additions between integers can be much more efficient than performing it between decimal numbers (fixed-point and especially floating-point).\n",
    "\n",
    "**Warning**: the assumption of folding is that transformations based on the commutative, associative, and distributive properties as well as on identity decompositions yield *exactly* equivalent operations.\n",
    "In digital arithmetic, this is usually not the case.\n",
    "For example, consider the possible ways to compute $\\sigma'' = \\sigma' = (\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma)/(\\epsilon_{\\mathbf{s}} \\sigma) = \\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma (1/\\epsilon_{\\mathbf{s}}) (1/\\sigma)$.\n",
    "Assuming that all the numbers in the expression are represented as floating-points, it might for example happen that\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\left( \\left( \\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\right) \\gamma \\right) \\left( \\frac{1}{\\epsilon_{\\mathbf{s}}} \\frac{1}{\\sigma} \\right)\n",
    "    \\neq\n",
    "    \\left( \\left( \\left( \\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\right) \\gamma \\right) \\frac{1}{\\epsilon_{\\mathbf{s}}} \\right) \\left( \\frac{1}{\\sigma} \\right) \\,.\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "### *Casting*\n",
    "\n",
    "Casting is the process by which digital data formats are converted into other data formats.\n",
    "An example of casting is transforming decimal numbers represented using a floating-point data format into decimal number represented using a fixed-point data format.\n",
    "It is usually the case that casting is a non-invertible transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b2b22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original value:                  5.571429\n",
      "Folded add-and-multiply value:   5.571429\n",
      "Folded multiply-and-add value:   5.571429\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# operation parameters\n",
    "n     = 128\n",
    "\n",
    "eps_x = 2.0\n",
    "x_hat = torch.randint(low=-1, high=1+1, size=(n,))\n",
    "x     = eps_x * x_hat\n",
    "\n",
    "eps_w = 1.0\n",
    "w_hat = torch.randint(low=-1, high=1+1, size=(n,))\n",
    "w     = eps_w * w_hat\n",
    "\n",
    "mi    = -3.0\n",
    "sigma = 0.5\n",
    "gamma = 3.0\n",
    "beta  = 1.5\n",
    "\n",
    "eps_s = 3.5\n",
    "\n",
    "\n",
    "def original(x, w, mi, sigma, gamma, beta, eps_s):\n",
    "    return (((torch.dot(x, w) - mi) / sigma) * gamma + beta) / eps_s\n",
    "\n",
    "\n",
    "def fold_dotaddandmultiply(eps_x, eps_w, mi, sigma, gamma, beta, eps_s):\n",
    "    \n",
    "    gammaprime  = eps_x * eps_w * gamma / (eps_s * sigma)\n",
    "    betaprime   = (sigma * beta - mi * gamma) / (eps_x * eps_w * gamma)\n",
    "\n",
    "    return betaprime, gammaprime\n",
    "\n",
    "\n",
    "def dotaddandmultiply(x_hat, w_hat, betaprime, gammaprime):\n",
    "    return (torch.dot(x_hat, w_hat) + betaprime) * gammaprime\n",
    "    \n",
    "\n",
    "def fold_dotmultiplyandadd(eps_x, eps_w, mi, sigma, gamma, beta, eps_s):\n",
    "\n",
    "    gammasecond = eps_x * eps_w * gamma / (eps_s * sigma)\n",
    "    betasecond  = (sigma * beta - mi * gamma) / (eps_s * sigma)\n",
    "    \n",
    "    return gammasecond, betasecond\n",
    "\n",
    "\n",
    "def dotmultiplyandadd(x_hat, w_hat, gammasecond, betasecond):\n",
    "    return torch.dot(x_hat, w_hat) * gammasecond + betasecond\n",
    "\n",
    "\n",
    "\n",
    "o   = original(x, w, mi, sigma, gamma, beta, eps_s)\n",
    "fam = dotaddandmultiply(x_hat, w_hat, *fold_dotaddandmultiply(eps_x, eps_w, mi, sigma, gamma, beta, eps_s))\n",
    "fma = dotmultiplyandadd(x_hat, w_hat, *fold_dotmultiplyandadd(eps_x, eps_w, mi, sigma, gamma, beta, eps_s))\n",
    "\n",
    "print(\"Original value:                {:10.6f}\".format(o))\n",
    "print(\"Folded add-and-multiply value: {:10.6f}\".format(fam))\n",
    "print(\"Folded multiply-and-add value: {:10.6f}\".format(fma))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ef934",
   "metadata": {},
   "source": [
    "## Bonus section: investigating sources of numerical discrepancy\n",
    "\n",
    "2-D arrays: index in batch $b$, feature $i$\n",
    "input $x[b]$, filter $w[i]$ ($i$-th row of weight matrix in linear layer)\n",
    "\n",
    "2-D + d arrays: index in batch $b$, feature $i$, position $(p_{0}, \\dots, p_{d-1})$\n",
    "\n",
    "coords = index[2:]\n",
    "\n",
    "padding = tuple(itertools.chain.from_iterable([(i, i) for i in mod.padding]))\n",
    "xpadded = nn.functional(x, padding)\n",
    "\n",
    "a_{i} = p_{i} * s_{i} --- b_{i} = p_{i} * s_{i} + k_{i}\n",
    "patch = x[b, :, a_{0}:b_{0}, ..., a_{d-1}:b_{d-1}]\n",
    "\n",
    "mu[i], sigma[i], weight[i], bias[i], eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def extract_fov_convnd(x:            torch.Tensor,\n",
    "                       convmod:      torch.nn.modules.conv._ConvNd,\n",
    "                       idx_in_batch: int,\n",
    "                       out_position: Tuple[int]):\n",
    "    \"\"\"Compute the patch (*field-of-view*, FOV) onto which the convolutional filter should be applied.\"\"\"\n",
    "\n",
    "    padding  = tuple(itertools.chain.from_iterable([(p, p) for p in convmod.padding]))\n",
    "    x_padded = torch.nn.functional.pad(x, padding)\n",
    "    \n",
    "    kernel   = convmod.kernel_size\n",
    "    stride   = convmod.stride\n",
    "    dilation = 1\n",
    "    spatial_slices = list(map(lambda t: slice(t[0] * t[1], t[0] * t[1] + t[2], dilation), zip(out_position, stride, kernel)))\n",
    "    \n",
    "    fov = x_padded[(slice(idx_in_batch, idx_in_batch + 1, 1), slice(None, None, 1)) + tuple(slice_ for slice_ in spatial_slices)]\n",
    "\n",
    "    return fov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_steps_convnd_bn_act_fakequantized(eps_x, x, convmod, bnmod, actmod, diff_coords):\n",
    "    \n",
    "    idx_in_batch = diff_coords[0]\n",
    "    out_channel  = diff_coords[1]\n",
    "    out_position = tuple(diff_coords[2:])\n",
    "    \n",
    "    # retrieve operands\n",
    "    fov    = extract_fov_convnd(x, convmod, idx_in_batch, out_position)\n",
    "\n",
    "    eps_w  = convmod.eps\n",
    "    weight = convmod.weight.data[out_channel]\n",
    "    \n",
    "    mi    = bnmod.running_mean[out_channel]\n",
    "    sigma = torch.sqrt(bnmod.running_var[out_channel] + bnmod.eps)\n",
    "    gamma = bnmod.weight[out_channel]\n",
    "    beta  = bnmod.bias[out_channel]\n",
    "    \n",
    "    eps_s = actmod.eps\n",
    "    lut   = torch.vstack((torch.hstack((torch.Tensor([-np.inf]), actmod.thresholds)), actmod.quant_levels))\n",
    "    \n",
    "#     # adjust for gamma's sign\n",
    "#     flip    = torch.sign(gamma)\n",
    "#     weight *= flip\n",
    "#     gamma  *= flip\n",
    "    \n",
    "    # fake-quantized ops\n",
    "    temp1 = torch.sum(fov * weight)\n",
    "    temp2 = temp1 * (gamma / sigma)\n",
    "    temp3 = temp2 + (beta - (gamma / sigma) * mi)\n",
    "    \n",
    "    cmp = (temp3 >= lut[0, :]).float()\n",
    "    idx = (lut.shape[-1] - 1) if cmp[-1] else torch.argmax(cmp[:-1] - cmp[1:])\n",
    "    temp4 = lut[1, idx]\n",
    "\n",
    "    temp5 = temp4 * eps_s\n",
    "    \n",
    "    print(temp1, \"({})\".format(temp1 / (eps_x * eps_w)), temp2, temp3, lut, temp4, temp5, \"({})\".format(temp5 / eps_s))\n",
    "    \n",
    "    \n",
    "def show_steps_convnd_lut_truequantized(x, convmod, lutmod, diff_coords):\n",
    "\n",
    "    idx_in_batch = diff_coords[0]\n",
    "    out_channel  = diff_coords[1]\n",
    "    out_position = tuple(diff_coords[2:])\n",
    "    \n",
    "    # retrieve operands\n",
    "    fov    = extract_fov_convnd(x, convmod, idx_in_batch, out_position)\n",
    "    weight = convmod.weight.data[out_channel]\n",
    "\n",
    "    lut   = torch.vstack((torch.hstack((torch.Tensor([-np.inf]), lutmod.tau[:, out_channel].squeeze())), lutmod.quant_levels))\n",
    "    \n",
    "    # true-quantized ops\n",
    "    temp1 = torch.sum(fov * weight)\n",
    "    cmp   = (temp1 >= lut[0, :]).float()\n",
    "    idx   = (lut.shape[-1] - 1) if cmp[-1] else torch.argmax(cmp[:-1] - cmp[1:])\n",
    "    temp2 = lut[1, idx]\n",
    "    \n",
    "    print(temp1, lut[0], temp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f5d09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
